{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1qFRy6S8qPGhkZ6UM7oWlpFg5qhJiH5qa?usp=sharing\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY: str = 'sk-XXXXXXXXXXXX' # Replace with your OpenAI API key\n",
    "\n",
    "!pip install openai &>/dev/null\n",
    "!pip install langchain &>/dev/null\n",
    "!pip install unstructured &>/dev/null \n",
    "!pip install python-magic-bin &>/dev/null\n",
    "!pip install chromadb &>/dev/null\n",
    "!pip install openai &>/dev/null\n",
    "!pip install gradio &>/dev/null\n",
    "\n",
    "import langchain\n",
    "import openai\n",
    "import os\n",
    "from langchain import OpenAI\n",
    "import langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import magic\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "source_requests = [\n",
    "                   'source?',\n",
    "                   'source',\n",
    "                   ]\n",
    "\n",
    "class Engine:\n",
    "  title: str = \"\"\n",
    "  qa = None\n",
    "  source_document = None\n",
    "\n",
    "  def setup_file(self, filepath):\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    try:\n",
    "      loader = DirectoryLoader('/', glob = filepath[1:])\n",
    "      documents = loader.load()\n",
    "      text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 0)\n",
    "      texts = text_splitter.split_documents(documents)\n",
    "      embeddings = OpenAIEmbeddings(openai_api_key = os.getenv('OPENAI_API_KEY'))\n",
    "      docsearch = Chroma.from_documents(texts, embeddings)\n",
    "      chain_type_kwargs = {\n",
    "          \"memory\": ConversationBufferMemory()\n",
    "      }\n",
    "      llm = ChatOpenAI(temperature = 0, \n",
    "                   verbose = True)\n",
    "      self.qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                           chain_type = 'stuff', \n",
    "                                           retriever=docsearch.as_retriever(),\n",
    "                                           chain_type_kwargs = chain_type_kwargs, \n",
    "                                           return_source_documents = True)\n",
    "      self.title = filepath\n",
    "    except:\n",
    "      raise Exception(\"Something went wrong when processing the txt file\")  \n",
    "\n",
    "engine = Engine()\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history\n",
    "\n",
    "def add_file(history, file):\n",
    "    engine.setup_file(file.name)\n",
    "    history = history + [(\"File succesfully uploaded. Prompt away! âœ…\", None)]\n",
    "    return history\n",
    "\n",
    "def bot(history, text):\n",
    "    if engine.qa:\n",
    "      response = engine.qa({'query': text})\n",
    "      if text.lower() in source_requests:\n",
    "        history[-1][1] = engine.source_document\n",
    "        yield history, \"\"\n",
    "      else:\n",
    "        history[-1][1] = \"\"\n",
    "        for info in re.split(\"(,|[\\n\\s+])\", response['result']):\n",
    "          history[-1][1] += info\n",
    "          time.sleep(0.075)\n",
    "          engine.source_document = \"\\\"\" + response['source_documents'][0].page_content + \" \\\"\"\n",
    "          yield history, \"\"\n",
    "    else:\n",
    "      history[-1][1] = \"Upload a document first\"\n",
    "      yield history, \"\"\n",
    "\n",
    "with gr.Blocks(theme = gr.themes.Soft()) as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\", label = \"TLDR the T&C\").style(height = 750)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.85):\n",
    "            txt = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Upload a T&C file (pdf or txt), then enter prompt\",\n",
    "            ).style(container=False)\n",
    "        with gr.Column(scale=0.15, min_width=0):\n",
    "            btn = gr.UploadButton(f\"ðŸ“„\", file_types=[\"text\", \"pdf\"])\n",
    "\n",
    "    txt.submit(add_text, [chatbot, txt], [chatbot]).then(\n",
    "        bot, [chatbot, txt], [chatbot, txt]\n",
    "    )\n",
    "    btn.upload(add_file, [chatbot, btn], [chatbot])\n",
    "\n",
    "demo.queue().launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
